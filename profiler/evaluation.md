# 测试样例
## 影响缓存效果的因素
- 不同分布的图数据集
- 图划分算法
- batch size
- 缓存架构
- 采样算法
## 图分布
分布倾斜程度的指标
## 静态缓存
> 优点：幂律分布图效果好  
> 缺点：batch_size增大效果急剧下降  

利用图的幂律分布，将最频繁访问的一部分节点缓存至GPU  
在多GPU的情况下，利用GPU间通信减少CPU和GPU之间的PCIe通信，降低对CPU资源的争用  
采样一律在GPU上进行，或者CPU/GPU混合采样
### 度
按度排序，获取节点在不同GPU上的权重，从头开始遍历，按照权重大小将节点缓存至不同的GPU  
此种方法的关键点在于节点在不同GPU上的分配策略  
我尝试获取batch的一阶邻居并统计其频率，作为在不同GPU上的权重才进行缓存分配，但差距不大
### 预采样
预采样若干轮统计节点访问频率，按不同GPU上的访问频率排序后，将最前面的部分缓存至显存  
这样的问题在于多个GPU之间缓存的数据重复率太高，不能很好地扩展缓存空间  
### PageRank
### 综合评价指标

## 动态缓存
### FIFO
- FIFO开销较低，特定情况下与PaGraph效果相当
- FIFO缓存替换每个minibatch进行一次
## 组合策略
- 进行预训练和对图的预处理来确定具体的缓存策略
- 动态缓存和静态缓存的比例划分指标 从图的幂律分布入手

## 测试
### 相邻若干个batch或子图间的重复性
batch重用，两级batch重用，扩展到无限级batch重用
### 再对比几个大型图数据集
### 新建一个可修改的配置文件 yaml或者其他

# 分区算法
- 分区算法的时间复杂度 是否随着图数据集规模的增大指数级增长
- 不同分区之间的冗余带来的开销
## Lin-Kernighan算法
## METIS
## LDG

# 数据集
- ogbn-products
- ogbn-papers100M
- Reddit
- WikiKG90M
- yelp

# todo
- [ ] **度排序后的前百分之多少，的度之和达到了度总数的百分之多少** 
- [ ] 预采样频率排序的百分比分布图
- [ ] 分区算法的 **数据集-开销**图
- [ ] 动态cache抽象成一个模块，静态cache抽象成一个模块
- [ ] 不同的测试函数单独放

# 建模
## 隔绝通信
- 不同分区之间节点会有冗余
  - 不同GPU的缓存内容有重复，需要有一个重复率指标，浪费了缓存空间，但是浪费是指发生在多GPU的，而它并不需要多GPU通信
  - 但是分区之后每个GPU上只需要缓存本分区内度排名靠前的节点，提高了数据的局部性，理论上会增加单个GPU的命中率
  - 这样的分区算法应该用在分布式训练，而不是单机多GPU
- 分区时间长

只要batch_size和采样参数相同，而无论图的大小，得到的子图中节点数量是相当的
pagraph说缓存20%的节点，是在每个GPU上缓存分区子图节点数量的20%，而nvlink缓存20%的节点，是在四个GPU上总共缓存20%的节点，每个GPU上交叉平均缓存了总图5%的节点
现在考虑实际情况，要保证每个GPU上缓存的节点个数是相等的，倒推出两种方案的缓存比例，并根据这个比例下的命中率来计算延迟，那么衡量延迟的重要参数就是缓存空间大小，也可视为显存大小
之所以用缓存空间大小来衡量延迟，是因为pagraph分区带来了较大的冗余，不同GPU上缓存的特征一定有重复，并且无法利用其他GPU上缓存的数据（无法完全避免跨分区访问）
nvlink中特征交叉分配的方案，在四个GPU上的命中率应该视作相同的，因为该方案只是进行了naive分区，并没有减少跨GPU访问
pagraph中分区的方案，四个GPU各自的命中率也是相似的
那么计算缓存节点比例时如何确定总节点数呢？
获取缓存比例只是为了计算单个GPU的命中率，那么节点总数就不重要了，全图按全图，子图按子图
## 无限级批次重用，理论上可以生成最优的特征调度序列
画个3d图先，明天，然后处理一下livejournel数据集
社区发现算法的应用
改七友的文档
## 目标 通过一种训练集分区算法，提高本地GPU的命中率，（但这势必也要修改缓存策略，动态策略是必须的）在尽可能缓存采样概率高的节点和缓存尽可能多的节点之间找到一个平衡点
csr topo和pagraph data的转换