# 数据集
## 介绍
以下均为图数据集，每个数据集中只包含一个图，将其分为训练集、测试集和验证集
## 对比
| 数据集          | 节点数  | 边数     | 特征维度 | 标签 |
| --------------- | ------- | -------- | -------- | ---- |
| ogbn-products   | 2449029 | 61859140 | 100      | 47   |
| ogbn-papers100M | xx      | xx       | xx       | xx   |
| reddit          | 232965  | 11606919 | 602      | 41   |
| WikiKG90M       | xx      | xx       | xx       | xx   |
| yelp            | xx      | xx       | xx       | xx   |
| livejournal     | xx      | xx       | xx       | xx   |
| pubmed          | 19717   | 44338    | 500      | 3    |
# 改进
## 图分区
- 多节点分布式训练时才需要图分区算法
- 社区发现
- 聚类
- METIS
- 分区算法的时间复杂度 是否随着图数据集规模的增大指数级增长
- 不同分区之间的冗余带来的开销
- Lin-Kernighan算法
- LDG
- PaGraph
- Hash
- Naive
## [训练集划分](#训练集分区算法)
- 基于全图最短路径的贪心分区算法
- 基于多源BFS的贪心分区算法
## 采样器
- 采样时增加被缓存节点的采样概率
- 多GPU并行采样
- 采样与训练分离
## 特征提取
- 最优特征调度序列
- 无限级batch重用
- 缓存
- 数据放置 
- 数据移动
## 训练
...
# 缓存方案
## 静态缓存
### 分析
#### 优点
幂律分布图效果好  
#### 缺点
batch_size增大效果急剧下降  

利用图的幂律分布，将最频繁访问的一部分节点缓存至GPU  
在多GPU的情况下，利用GPU间通信减少CPU和GPU之间的PCIe通信，降低对CPU资源的争用  
采样一律在GPU上进行，或者CPU/GPU混合采样
### 策略
#### 度
按度排序，获取节点在不同GPU上的权重，从头开始遍历，按照权重大小将节点缓存至不同的GPU  
此种方法的关键点在于节点在不同GPU上的分配策略  
我尝试获取batch的一阶邻居并统计其频率，作为在不同GPU上的权重才进行缓存分配，但差距不大
#### 预采样
预采样若干轮统计节点访问频率，按不同GPU上的访问频率排序后，将最前面的部分缓存至显存  
这样的问题在于多个GPU之间缓存的数据重复率太高，不能很好地扩展缓存空间  
#### PageRank
#### 综合评价指标

## 动态缓存
### 分析
...
### 策略
#### FIFO
- FIFO开销较低，特定情况下与PaGraph效果相当
- FIFO缓存替换每个minibatch进行一次
## 组合缓存
### 分析
- 进行预训练和对图的预处理来确定具体的缓存策略
- 动态缓存和静态缓存的比例划分指标 从图的幂律分布入手
## 影响缓存效果的因素
- 不同分布的图数据集
  - 分布倾斜度的指标（幂律分布、齐夫定律）
- 图划分算法
- batch size
- 缓存架构
- 采样算法
# 建模
## 隔绝通信的图分区
- 不同分区之间节点会有冗余
  - 不同GPU的缓存内容有重复，需要有一个重复率指标，浪费了缓存空间，但是浪费是指发生在多GPU的，而它并不需要多GPU通信
  - 但是分区之后每个GPU上只需要缓存本分区内度排名靠前的节点，提高了数据的局部性，理论上会增加单个GPU的命中率
  - 这样的分区算法应该用在分布式训练，而不是单机多GPU
- 分区时间长
## nvlink通信
- 只要batch_size和采样参数相同，而无论图的大小，得到的子图中节点数量是相当的
- pagraph说缓存20%的节点，是在每个GPU上缓存分区子图节点数量的20%，而nvlink缓存20%的节点，是在四个GPU上总共缓存20%的节点，每个GPU上交叉平均缓存了总图5%的节点
- 现在考虑实际情况，要保证每个GPU上缓存的节点个数是相等的，倒推出两种方案的缓存比例，并根据这个比例下的命中率来计算延迟，那么衡量延迟的重要参数就是缓存空间大小，也可视为显存大小
- 之所以用缓存空间大小来衡量延迟，是因为pagraph分区带来了较大的冗余，不同GPU上缓存的特征一定有重复，并且无法利用其他GPU上缓存的数据（无法完全避免跨分区访问）
- nvlink中特征交叉分配的方案，在四个GPU上的命中率应该视作相同的，因为该方案只是进行了naive分区，并没有减少跨GPU访问
- pagraph中分区的方案，四个GPU各自的命中率也是相似的
- 那么计算缓存节点比例时如何确定总节点数呢？
- 获取缓存比例只是为了计算单个GPU的命中率，那么节点总数就不重要了，全图按全图，子图按子图
## 无限级批次重用，理论上可以生成最优的特征调度序列
画个3d图先，明天，然后处理一下livejournel数据集
社区发现算法的应用
改七友的文档
# 训练集分区算法
## 目标
通过一种训练集分区算法，提高本地GPU的命中率，（但这势必也要修改缓存策略，动态策略是必须的）  
在尽可能缓存采样概率高的节点和缓存尽可能多的节点之间找到一个平衡点
近似最邻近图

## 训练集分区算法测试
reddit数据集，四个分区  
分别测试每个gpu上的采样子图节点数，图节点数仍为全图节点数  
在每个GPU上分别进行预采样统计，并排序，按照一定比例缓存至GPU，注意此处各分区缓存的数量可能不同  
训练集节点分区的时候考虑度


quiver0.2.0
# 测试
- 相邻若干个batch或子图间的重复性
- 不同策略、不同数据集、不同比例、batch size的命中率
- 模型计算和数据加载的占比
- 训练集分区算法的nvlink命中率
- 测试几个大型图数据集
- 测试使用训练集划分算法下不同GPU的数据在所有GPU上的命中率
# TODO
- [ ] 新建一个可修改的配置文件 yaml或者其他
- [ ] 新建一个独立项目，负责训练集分区算法、图数据集预处理、csrtopo定义
- [ ] **度排序后的前百分之多少，的度之和达到了度总数的百分之多少** 
- [ ] 预采样频率排序的百分比分布图
- [ ] 分区算法的 **数据集-开销**图
- [ ] 动态cache抽象成一个模块，静态cache抽象成一个模块
- [ ] 不同的测试函数单独放
- [ ] csr topo和pagraph data的转换
